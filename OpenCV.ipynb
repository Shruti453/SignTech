{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5e3662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 75\n",
    "ROI_bottom = 325\n",
    "ROI_right = 250\n",
    "ROI_left = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de7b552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "def cal_accum_avg(frame, accumulated_weight=0.5):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e907d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    cv2.imshow(\"diff\", diff)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    cv2.imshow(\"thresholded_segment_hand\", thresholded)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81055f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "num_frames = 0\n",
    "element = 1\n",
    "num_imgs_taken = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)\n",
    "    canny_gray = cv2.Canny(gray_frame, 100,200, apertureSize=3, L2gradient=True)\n",
    "#     cv2.imshow('grayframe', gray_frame)\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "#             cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for \" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        # Checking if hand is actually detected by counting number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "#             cv2.imshow(\"thresholded\", thresholded)\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            cv2.drawContours(gray_frame, hand_segment, -1, (255, 0, 0),1)\n",
    "            canny_gray = cv2.Canny(gray_frame, 100, 200,  apertureSize=3, L2gradient=True)\n",
    "            canny_white = cv2.Canny(roi, 100, 125)\n",
    "            \n",
    "        \n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\" images for \" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image_elif\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            cv2.drawContours(gray_frame, hand_segment, -1, (255, 0, 0),1)\n",
    "            canny_gray = cv2.Canny(gray_frame, 100,200, apertureSize=3,L2gradient=True)\n",
    "            canny_white = cv2.Canny(roi, 100,125)\n",
    "            \n",
    "            \n",
    "#             cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "#             #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "#             cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image_else\", thresholded)\n",
    "#             if num_imgs_taken <= 300:\n",
    "#                 cv2.imwrite(r\"D:\\\\Desktop\\ML\\\\sign_language\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "# #                 cv2.imwrite(r\"D:\\Desktop\\ML\\sign_language\\gesture\"+\"\\\\\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "#             else:\n",
    "#                 break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "    cv2.imshow(\"gray_frame\", gray_frame)\n",
    "    cv2.imshow(\"canny_gray\", canny_gray)\n",
    "    cv2.imshow(\"canny_white\", canny_white)\n",
    "    \n",
    "\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7774884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing camera & destroying all the windows...\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59069961",
   "metadata": {},
   "source": [
    "Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54de51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "# from keras.optimizers import Adam, SGD\n",
    "from keras.optimizers import gradient_descent_v2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b9096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3010 images belonging to 10 classes.\n",
      "Found 410 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = r'D:\\Desktop\\ML\\sign_language\\gesture\\contours\\train'\n",
    "test_path = r'D:\\Desktop\\ML\\sign_language\\gesture\\contours\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), classes =['1','2','3','4','5','6','7','8','9','10'],  class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), classes = ['1','2','3','4','5','6','7','8','9','10'], class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ebf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1a792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(64,activation =\"relu\"),\n",
    "    Dense(128,activation =\"relu\"),\n",
    "    Dense(128,activation =\"relu\"),\n",
    "    Dense(10,activation =\"softmax\")\n",
    "])\n",
    "\n",
    "# model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(64,activation =\"relu\"))\n",
    "# model.add(Dense(128,activation =\"relu\"))\n",
    "# #model.add(Dropout(0.2))\n",
    "# model.add(Dense(128,activation =\"relu\"))\n",
    "# #model.add(Dropout(0.3))\n",
    "# model.add(Dense(10,activation =\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2064518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 414,346\n",
      "Trainable params: 414,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a8bdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301/301 [==============================] - 31s 98ms/step - loss: 0.6721 - accuracy: 0.8924 - val_loss: 1.8603 - val_accuracy: 0.7317 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 19s 64ms/step - loss: 5.1825e-04 - accuracy: 1.0000 - val_loss: 1.5280 - val_accuracy: 0.7756 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 17s 55ms/step - loss: 8.8749e-05 - accuracy: 1.0000 - val_loss: 1.4853 - val_accuracy: 0.8171 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 17s 57ms/step - loss: 1.8161e-05 - accuracy: 1.0000 - val_loss: 1.5163 - val_accuracy: 0.8171 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 16s 55ms/step - loss: 1.3325e-05 - accuracy: 1.0000 - val_loss: 1.5218 - val_accuracy: 0.8171 - lr: 2.0000e-04\n",
      "loss of 0.038020022213459015; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "# model.compile(optimizer=gradient_descent_v2.SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9abe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.6720829010009766, 0.0005182483000680804, 8.874863124219701e-05, 1.81614414032083e-05, 1.3324547580850776e-05], 'accuracy': [0.8923587799072266, 1.0, 1.0, 1.0, 1.0], 'val_loss': [1.8603191375732422, 1.528037190437317, 1.4853137731552124, 1.5163371562957764, 1.5217989683151245], 'val_accuracy': [0.7317073345184326, 0.7756097316741943, 0.8170731663703918, 0.8170731663703918, 0.8170731663703918], 'lr': [0.001, 0.001, 0.001, 0.001, 0.00020000001]}\n",
      "loss of 0.07951419055461884; accuracy of 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "model.save('best_model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r\"best_model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289748ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "[[3.32638336e-07 3.27940941e-09 1.43053194e-10 9.99961972e-01\n",
      "  3.64414518e-05 1.08064546e-06 4.54704931e-08 7.04018549e-13\n",
      "  1.23187164e-07 2.30199262e-14]\n",
      " [6.44561986e-13 4.24568353e-10 1.65926431e-12 2.18683871e-09\n",
      "  3.35525124e-06 5.26517985e-09 2.39526816e-08 9.99996543e-01\n",
      "  4.66546695e-08 3.32603989e-08]\n",
      " [7.42280406e-08 9.99962330e-01 5.74957994e-06 1.39575929e-09\n",
      "  7.78804528e-15 3.15085381e-05 3.87593541e-10 1.37405720e-09\n",
      "  3.74402987e-07 2.98811635e-11]\n",
      " [3.35418875e-08 1.36852257e-11 7.93022592e-12 9.99997497e-01\n",
      "  2.06992740e-06 1.73413994e-08 9.29181798e-09 2.35585141e-12\n",
      "  4.13324273e-07 1.93383530e-15]\n",
      " [9.10208564e-06 2.87864041e-05 9.58254755e-01 6.08843002e-05\n",
      "  1.18200704e-02 2.98152547e-02 1.16862884e-08 5.03899367e-10\n",
      "  1.12098814e-05 2.05788044e-08]\n",
      " [1.37082335e-13 8.00901984e-11 2.43371865e-13 1.37909434e-10\n",
      "  5.92585572e-08 3.16523724e-10 2.34926900e-09 9.99999881e-01\n",
      "  1.47307437e-08 1.15582575e-08]\n",
      " [2.39699549e-08 2.25894494e-11 2.76346567e-11 9.99996424e-01\n",
      "  3.13872033e-06 1.63709970e-07 1.32766033e-07 5.72887701e-12\n",
      "  1.51489004e-07 3.51181935e-15]\n",
      " [6.16660031e-07 2.29651846e-06 3.89391580e-03 3.11593176e-03\n",
      "  9.26376581e-02 8.99254262e-01 1.33831591e-05 7.88766542e-04\n",
      "  1.95703164e-04 9.74658615e-05]\n",
      " [5.80539394e-09 1.54280751e-05 3.44014353e-07 2.21976326e-04\n",
      "  4.01368976e-04 4.49820310e-01 5.24442792e-01 2.48152949e-02\n",
      "  2.79255153e-04 3.16099408e-06]\n",
      " [1.77224302e-09 5.26364056e-06 1.70276398e-05 1.64241974e-05\n",
      "  4.32667548e-05 9.99193728e-01 8.31826037e-06 6.69973670e-04\n",
      "  3.13633973e-05 1.46325510e-05]]\n",
      "4\n",
      "8\n",
      "2\n",
      "4\n",
      "3\n",
      "8\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "4\n",
      "8\n",
      "2\n",
      "4\n",
      "3\n",
      "8\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n",
      "(10, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6720829010009766,\n",
       "  0.0005182483000680804,\n",
       "  8.874863124219701e-05,\n",
       "  1.81614414032083e-05,\n",
       "  1.3324547580850776e-05],\n",
       " 'accuracy': [0.8923587799072266, 1.0, 1.0, 1.0, 1.0],\n",
       " 'val_loss': [1.8603191375732422,\n",
       "  1.528037190437317,\n",
       "  1.4853137731552124,\n",
       "  1.5163371562957764,\n",
       "  1.5217989683151245],\n",
       " 'val_accuracy': [0.7317073345184326,\n",
       "  0.7756097316741943,\n",
       "  0.8170731663703918,\n",
       "  0.8170731663703918,\n",
       "  0.8170731663703918],\n",
       " 'lr': [0.001, 0.001, 0.001, 0.001, 0.00020000001]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {'1':'One','2':'Two','3':'Three','4':'Four','5':'Five','6':'Six','7':'Seven','8':'Eight','9':'Nine', '10':'Ten'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "print(predictions)\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(np.argmax(i)+1)\n",
    "#     print(word_dict[str(np.argmax(i))], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "#     print(word_dict[np.argmax(i)], end='   ')\n",
    "    print(np.argmax(i)+1)\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5caaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
